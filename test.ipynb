{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Import der benötigten Bibiliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wave\n",
    "import sys\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Activation\n",
    "from keras.callbacks import EarlyStopping \n",
    "from sklearn.model_selection import train_test_split as ts \n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "from sklearn.svm import SVC as svc\n",
    "# import soundfile as sf\n",
    "from tsfresh import extract_relevant_features\n",
    "from tsfresh.examples.robot_execution_failures import load_robot_execution_failures\n",
    "from tsfresh import extract_features\n",
    "# import soundfile as sf\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from openpyxl import Workbook\n",
    "import wave\n",
    "import random\n",
    "from pydub import AudioSegment\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Data Pre Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.1 Extraktion des Features MFCC = Koeffizient zur Kompakten Darstellung des Klangs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"WomenSequences\\mor\\do you ever feel like hard h1.wav\"\n",
    "def extract_mfcc(file_name, n_mfcc=13):\n",
    "    y, sr = librosa.load(file_name)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    df_MFCC = pd.DataFrame(mfcc)\n",
    "    for i in range(df_MFCC.shape[1]):\n",
    "        df_MFCC = df_MFCC.rename(columns={i: f\"MFCC{i+1}\"})\n",
    "    return df_MFCC\n",
    "\n",
    "\n",
    "df = extract_mfcc(path)\n",
    "df = pd.DataFrame(df)\n",
    "df.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.2 Plotten der MFCC Werte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2.1 Extraction und Berechnung des Features Zero Crossing Rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_zcr(file_name):\n",
    "    y, sr = librosa.load(file_name)\n",
    "    zcr = librosa.feature.zero_crossing_rate(y)\n",
    "    df_ZCR = pd.DataFrame(zcr)\n",
    "    for i in range(df_ZCR.shape[1]):\n",
    "        df_ZCR = df_ZCR.rename(columns={i: f\"Zero Crossing Rate{i+1}\"})\n",
    "    return df_ZCR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_zcr(file_name):\n",
    "    y, sr = librosa.load(file_name)\n",
    "    zcr = librosa.feature.zero_crossing_rate(y)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(zcr[0])\n",
    "    plt.title('Zero Crossing Rate')\n",
    "    plt.show()\n",
    "\n",
    "zcr = extract_zcr(path)\n",
    "dfzcr = pd.DataFrame(zcr)\n",
    "dfzcr.head()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2.2 Plotten der Zero Crossing Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_zcr(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3.1 Berechnung und Extraction des Features Tonstärke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_loudness(file_name):\n",
    "    y, sr = librosa.load(file_name)\n",
    "    S = np.abs(librosa.stft(y))\n",
    "    loudness = librosa.feature.spectral_bandwidth(S=S)\n",
    "    df_Loudness = pd.DataFrame(loudness)\n",
    "    for i in range(df_Loudness.shape[1]):\n",
    "        df_Loudness = df_Loudness.rename(columns={i: f\"Tonstärke{i+1}\"})\n",
    "    return df_Loudness\n",
    "\n",
    "def plot_loudness(file_name):\n",
    "    y, sr = librosa.load(file_name)\n",
    "    S = np.abs(librosa.stft(y))\n",
    "    loudness = librosa.feature.spectral_bandwidth(S=S)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(loudness[0])\n",
    "    plt.title('Loudness')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "dfLoudness = extract_loudness(path)\n",
    "dfLoudness.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3.2 Plotten der Tonstärke "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_loudness(path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4.1 Extraction und Berechnung des Spcetral Kontrasts\n",
    "#### Definition: Maß für die Energiedifferenz zwischen den Frequenzbändern eines Audiosignals. Verwendet zur Klangfarben Charakterisierung eines Audiosignals. \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_snr(file_name):\n",
    "    y, sr = librosa.load(file_name)\n",
    "    S = np.abs(librosa.stft(y))\n",
    "    snr = librosa.feature.spectral_contrast(S=S)\n",
    "    dfsnr = pd.DataFrame(snr)\n",
    "    for i in range(dfsnr.shape[1]):\n",
    "        dfsnr = dfsnr.rename(columns={i: f\"Spektral Kontrast{i+1}\"})\n",
    "    return dfsnr\n",
    "\n",
    "\n",
    "\n",
    "dfSnR = extract_snr(path)\n",
    "dfSnR.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4.2 Plotten des Spektral Kontrasts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_snr(file_name):\n",
    "    snr = extract_snr(file_name)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(snr, aspect='auto', origin='lower', cmap='coolwarm')\n",
    "    plt.colorbar()\n",
    "    plt.ylabel('Frequency bands')\n",
    "    plt.xlabel('Time (frames)')\n",
    "    plt.title('Spectral contrast')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_snr(path)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5.1 Berechnung der Bandbreite eines Audiosignals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_bandwidth(file_name):\n",
    "#     y, sr = librosa.load(file_name)\n",
    "#     S = np.abs(librosa.stft(y))\n",
    "#     bandwidth = librosa.feature.spectral_bandwidth(S=S)\n",
    "#     return bandwidth\n",
    "\n",
    "def extract_bandwidth(file_name):\n",
    "    y, sr = librosa.load(file_name)\n",
    "    S = np.abs(librosa.stft(y))\n",
    "    bandwidth = librosa.feature.spectral_bandwidth(S=S)\n",
    "    dfBandwith = pd.DataFrame(bandwidth)\n",
    "    for i in range(dfBandwith.shape[1]):\n",
    "        dfBandwith = dfBandwith.rename(columns={i: f\"Bandbreite{i+1}\"})\n",
    "    return dfBandwith\n",
    "\n",
    "dfBandwith = extract_bandwidth(path)\n",
    "dfBandwith.head()\n",
    "\n",
    "\n",
    "# dfBandwith.to_csv(\"dfBandwith.csv\")\n",
    "# df = pd.read_csv(\"dfBandwith.csv\")\n",
    "# df = pd.DataFrame(df)\n",
    "# for i in range(df.shape[1]):\n",
    "#     df = df.rename(columns={f'{i}': f\"Bandbreite{i}\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5.2 Plotten der Bandbreite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bandwidth(file_name):\n",
    "    bandwidth = extract_bandwidth(file_name)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(bandwidth, aspect='auto', origin='lower', cmap='coolwarm')\n",
    "    plt.colorbar()\n",
    "    plt.ylabel('Frequency bands')\n",
    "    plt.xlabel('Time (frames)')\n",
    "    plt.title('Spectral bandwidth')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_bandwidth(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bandwith = extract_bandwidth(path)\n",
    "df_bandwith = pd.DataFrame(df_bandwith)\n",
    "df_loudness = extract_loudness(path)\n",
    "df_loudness = pd.DataFrame(df_loudness)\n",
    "df_mfcc = extract_mfcc(path)\n",
    "df_mfcc = pd.DataFrame(df_mfcc)\n",
    "df_snr = extract_snr(path)\n",
    "df_snr =    pd.DataFrame(df_snr)\n",
    "df_zcr = extract_zcr(path)\n",
    "df_zcr = pd.DataFrame(df_zcr)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_bandwith.head()\n",
    "\n",
    "\n",
    "\n",
    "# extract_loudness(path)\n",
    "# extract_mfcc(path)\n",
    "# extract_snr(path)\n",
    "# extract_zcr(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loudness.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mfcc.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snr.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zcr.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Machine Learning Modell trainieren"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.1 Daten in eine Form bringen mit dem das Machine Learning Model trainiert werden kann"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1 Aufsplitten von WAV Dateien, in 3 Sekunden Sequenzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def split_wav(destinationPath, wav_file, segment_length=3000):\n",
    "    audio = AudioSegment.from_wav(wav_file)\n",
    "\n",
    "    for i in range(0, len(audio), segment_length):\n",
    "        segment = audio[i:i+segment_length]\n",
    "        segment.export(f\"{destinationPath}segment__{i-10//segment_length}.wav\", format=\"wav\")\n",
    "        \n",
    "\n",
    "\n",
    "split_wav(\"C:/Users/busse/ManOderFrau/menAudios/\",\"C:/Users/busse/ManOderFrau/menAudios/1.wav\", segment_length=3000)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getDuration(input_file_path,duration):\n",
    "    with wave.open(input_file_path, 'rb') as input_wav:\n",
    "        n_channels = input_wav.getnchannels()\n",
    "        sample_width = input_wav.getsampwidth()\n",
    "        frame_rate = input_wav.getframerate()\n",
    "        n_frames = input_wav.getnframes()\n",
    "\n",
    "        n_frames_duration = int(frame_rate * duration)\n",
    "        return n_frames_duration \n",
    "    \n",
    "def get_n_frames_duration (input_file_path,duration):\n",
    "    with wave.open(input_file_path, 'rb') as input_wav:\n",
    "        n_channels = input_wav.getnchannels()\n",
    "        sample_width = input_wav.getsampwidth()\n",
    "        frame_rate = input_wav.getframerate()\n",
    "        n_frames = input_wav.getnframes()\n",
    "\n",
    "        n_frames_duration = int(frame_rate * duration)\n",
    "        return n_frames_duration \n",
    "    \n",
    "\n",
    "def extract_random_sequence(input_file_path, output_file_path, duration=3):\n",
    "    with wave.open(input_file_path, 'rb') as input_wav:\n",
    "        n_channels = input_wav.getnchannels()\n",
    "        sample_width = input_wav.getsampwidth()\n",
    "        frame_rate = input_wav.getframerate()\n",
    "        n_frames = input_wav.getnframes()\n",
    "\n",
    "        n_frames_duration = int(frame_rate * duration)\n",
    "\n",
    "        if n_frames_duration > n_frames:\n",
    "            raise ValueError(f\"Die angegebene Dauer ({duration} Sekunden) ist länger als die Gesamtdauer der Datei ({n_frames / frame_rate} Sekunden).\")\n",
    "\n",
    "        start_frame = random.randint(0, n_frames - n_frames_duration)\n",
    "\n",
    "        input_wav.setpos(start_frame)\n",
    "        frames = input_wav.readframes(n_frames_duration)\n",
    "\n",
    "\n",
    "    with wave.open(output_file_path, 'wb') as output_wav:\n",
    "        output_wav.setparams((n_channels, sample_width, frame_rate, n_frames_duration, 'NONE', 'not compressed'))\n",
    "        output_wav.writeframes(frames)\n",
    "\n",
    "\n",
    "def get_n_frames(input_file_path):\n",
    "      with wave.open(input_file_path, 'rb') as input_wav:\n",
    "        n_channels = input_wav.getnchannels()\n",
    "        sample_width = input_wav.getsampwidth()\n",
    "        frame_rate = input_wav.getframerate()\n",
    "        n_frames = input_wav.getnframes()\n",
    "\n",
    "        # n_frames_duration = int(frame_rate * duration)\n",
    "        return n_frames \n",
    "\n",
    "\n",
    "def split_multiple_frames(input_file_path,output_file_path,duration=3):\n",
    "    i = 0\n",
    "    for files in os.listdir(input_file_path):\n",
    "\n",
    "        if files.endswith(\".wav\"):\n",
    "                \n",
    "            # if get_n_frames_duration(\"longMenAudios/\" + files,duration=3) > get_n_frames(\"longMenAudios/\" + files):\n",
    "                extract_random_sequence(input_file_path+ files,output_file_path  + f\"Random{i}duration_{duration}_.wav\",duration=3)\n",
    "                \n",
    "                print(f\"Random{i}duration_{duration}\")\n",
    "                i = i+1\n",
    "\n",
    "\n",
    "\n",
    "def rename_data(file_path=\"MenSequences\" or \"WomenSequences\",files_to_rename=\"Random\"):\n",
    "    i = 0\n",
    "    for files in os.listdir(file_path):\n",
    "        if files.endswith(\".wav\"):\n",
    "            new_name = f\"{files_to_rename}_{i}.wav\"\n",
    "            os.rename(os.path.join(file_path, files), os.path.join(file_path, new_name))\n",
    "            i += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# split_multiple_frames(\"C:/Users/busse/ManOderFrau/MenMP4/\",output_file_path=\"C:/Users/busse/ManOderFrau/MenMP4/\",duration=3)\n",
    "\n",
    "# mp4_to_wav(\"C:/Users/busse/ManOderFrau/MenMP4/Microphone.mp4\",\"C:/Users/busse/ManOderFrau/MenMP4/Test23.06.2023.wav\")\n",
    "# split_multiple_frames(\"C:/Users/busse/ManOderFrau/MenMP4/\",\"C:/Users/busse/ManOderFrau/menData/\",duration=3)\n",
    "split_multiple_frames(\"C:/Users/busse/ManOderFrau/MenMP4/\",\"C:/Users/busse/ManOderFrau/MenMp4/\",duration=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1.1 Daten nur von einer Frau labeln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandwidth_vec = df_bandwith.values.flatten()\n",
    "zcr_vec = df_zcr.values.flatten()\n",
    "sc_vec = df_snr.values.flatten()\n",
    "mfcc_vec = df_mfcc.values.flatten()\n",
    "chroma_vec = df_loudness.values.flatten()\n",
    "\n",
    "# X = np.hstack([bandwidth_vec, zcr_vec, sc_vec, mfcc_vec, chroma_vec])\n",
    "\n",
    "df_bandwith = pd.DataFrame(index=[0], columns=['Bandbreite'])\n",
    "df_loudness = pd.DataFrame(index=[1], columns= ['Tonstärke'])\n",
    "df_snr = pd.DataFrame(index=[2], columns=   ['Spektral Kontrast']) \n",
    "df_zcr = pd.DataFrame(index=[3], columns= ['Zero Crossing Rate'])\n",
    "df_mfcc = pd.DataFrame(index=[4], columns= ['MFCC'])\n",
    "\n",
    "# Speichere den Vektor als Liste in der Zelle des DataFrames\n",
    "# df_bandwith.at[0, 'Bandbreite'] = bandwidth_vec.tolist()\n",
    "# df_loudness.at[4,'Tonstärke'] = chroma_vec.tolist()\n",
    "# df_snr.at[0, 'Spektral Kontrast'] = sc_vec.tolist()\n",
    "# df_zcr.at[3, 'Zero Crossing Rate'] = zcr_vec.tolist()\n",
    "# df_mfcc.at[4, 'MFCC'] = mfcc_vec.tolist()\n",
    "\n",
    "\n",
    "# df_loudness\n",
    "# df_snr\n",
    "# df_zcr\n",
    "# df_mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bandwith.at[0, 'Bandbreite'] = bandwidth_vec.tolist()\n",
    "df_bandwith['id']=range(1,len(df_bandwith)+1)\n",
    "\n",
    "df_bandwith\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loudness.at[1,'Tonstärke'] = chroma_vec.tolist()\n",
    "df_loudness['id'] = range(1,len(df_loudness)+1)\n",
    "\n",
    "df_loudness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_snr.at[2, 'Spektral Kontrast'] = sc_vec.tolist()\n",
    "df_snr['id'] = range(1,len(df_snr)+1)\n",
    "df_snr\n",
    "# df_zcr[\"id\"] = range(1,len(df_zcr)+1)\n",
    "# df_mfcc [\"id\"] = range(1,len(df_mfcc)+1)\n",
    "\n",
    "# df_snr.at[0, 'Spektral Kontrast'] = sc_vec.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zcr.at[3, 'Zero Crossing Rate'] = zcr_vec.tolist()\n",
    "df_zcr['id'] = range(1,len(df_zcr)+1)\n",
    "df_zcr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mfcc.at[4, 'MFCC'] = mfcc_vec.tolist()\n",
    "df_mfcc ['id'] = range(1,len(df_mfcc)+1)\n",
    "df_mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mergeBandwithLoudness = pd.merge(df_bandwith, df_loudness, on='id')\n",
    "\n",
    "\n",
    "merge1 = pd.merge(df_mfcc,df_zcr,on='id')\n",
    "merge1\n",
    "\n",
    "# mergeSnrZcr = pd.merge(df_snr,df_zcr,on='id')\n",
    "# mergeBandwithLoudness\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge2 = pd.merge(merge1, df_loudness, on='id')\n",
    "merge2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge3 = pd.merge(merge2, df_snr,on='id')\n",
    "merge3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge4 = pd.merge(merge3, df_bandwith,on=\"id\")\n",
    "merge4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge4['Label'] = ['Frau']\n",
    "merge4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1.2 Daten nur von einem Mann labeln "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import *\n",
    "from pydub import AudioSegment\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_features_df_csv(nameOfCsv,ordner_path):\n",
    "    i = 0\n",
    "    for file in os.listdir(ordner_path):\n",
    "        print(file)\n",
    "        if file.endswith('.wav'):\n",
    "            newPath = ordner_path + \"/\" + file\n",
    "            df_mfcc2 = extract_mfcc(newPath)\n",
    "            df_loudness2 = extract_loudness(newPath)\n",
    "            df_snr2 = extract_snr(newPath)\n",
    "            df_zcr2 = extract_zcr(newPath)\n",
    "            df_bandwith2 = extract_bandwidth(newPath)\n",
    "            df_bandwith2 = pd.DataFrame(df_bandwith2)\n",
    "            df_loudness2 = pd.DataFrame(df_loudness2)\n",
    "            df_snr2 = pd.DataFrame(df_snr2)\n",
    "            df_zcr2 = pd.DataFrame(df_zcr2)\n",
    "            df_mfcc2 = pd.DataFrame(df_mfcc2)\n",
    "            df_bandwith2 = df_bandwith2.iloc[:, :80]\n",
    "            df_loudness2 = df_loudness2.iloc[:, :80]\n",
    "            df_snr2 = df_snr2.iloc[:, :80]\n",
    "            df_zcr2 = df_zcr2.iloc[:, :80]\n",
    "            df_mfcc2 = df_mfcc2.iloc[:, :80]\n",
    "\n",
    "            bandwidth_vec2 = df_bandwith2.values.flatten()\n",
    "            zcr_vec2 = df_zcr2.values.flatten()\n",
    "            sc_vec2 = df_snr2.values.flatten()\n",
    "            mfcc_vec2 = df_mfcc2.values.flatten()\n",
    "            chroma_vec2 = df_loudness2.values.flatten()\n",
    "\n",
    "            df_bandwith2 = pd.DataFrame(index=[0], columns=['Bandbreite'])\n",
    "            df_loudness2 = pd.DataFrame(index=[1], columns= ['Tonstärke'])\n",
    "            df_snr2 = pd.DataFrame(index=[2], columns=   ['Spektral Kontrast']) \n",
    "            df_zcr2 = pd.DataFrame(index=[3], columns= ['Zero Crossing Rate'])\n",
    "            df_mfcc2 = pd.DataFrame(index=[4], columns= ['MFCC'])\n",
    "\n",
    "\n",
    "            df_bandwith2.at[0, 'Bandbreite'] = bandwidth_vec2.tolist()\n",
    "            df_loudness2.at[1,'Tonstärke'] = chroma_vec2.tolist()\n",
    "            df_snr2.at[2, 'Spektral Kontrast'] = sc_vec2.tolist()\n",
    "            df_zcr2.at[3, 'Zero Crossing Rate'] = zcr_vec2.tolist()\n",
    "            df_mfcc2.at[4, 'MFCC'] = mfcc_vec2.tolist()\n",
    "\n",
    "            df_bandwith2['id']=range(1,len(df_bandwith2)+1)\n",
    "            df_loudness2['id']=range(1,len(df_loudness2)+1)\n",
    "            df_snr2['id']=range(1,len(df_snr2)+1)\n",
    "            df_zcr2['id']=range(1,len(df_zcr2)+1)\n",
    "            df_mfcc2['id']=range(1,len(df_mfcc2)+1)\n",
    "\n",
    "\n",
    "\n",
    "            mergeFirst = pd.merge(df_mfcc2,df_zcr2,on='id')\n",
    "            mergeSecond = pd.merge(mergeFirst, df_loudness2, on='id')\n",
    "            mergeThird = pd.merge(mergeSecond, df_snr2,on='id')\n",
    "            mergeForth = pd.merge(mergeThird, df_bandwith2,on='id')\n",
    "            mergeForth['Frau/Mann'] = ['Frau']\n",
    "            mergeForth.to_csv(f\"{nameOfCsv}{i}.csv\")\n",
    "\n",
    "            \n",
    "            i+=1\n",
    "\n",
    "\n",
    "\n",
    "def get_features_df_excel(ordner_path, destinationPath, nameOfXLSX, numberOfXLSXData, labelType, numberOfColumns):\n",
    "    i = 0\n",
    "    for file in os.listdir(ordner_path):\n",
    "        print(file)\n",
    "        if i == numberOfXLSXData: \n",
    "            return\n",
    "        if file.endswith('.wav'):\n",
    "            print(ordner_path + file)\n",
    "            newPath = ordner_path + \"/\" + file\n",
    "            df_mfcc2 = extract_mfcc(newPath)\n",
    "            df_loudness2 = extract_loudness(newPath)\n",
    "            df_snr2 = extract_snr(newPath)\n",
    "            df_zcr2 = extract_zcr(newPath)\n",
    "            df_bandwith2 = extract_bandwidth(newPath)\n",
    "            df_bandwith2 = pd.DataFrame(df_bandwith2)\n",
    "            df_loudness2 = pd.DataFrame(df_loudness2)\n",
    "            df_snr2 = pd.DataFrame(df_snr2)\n",
    "            df_zcr2 = pd.DataFrame(df_zcr2)\n",
    "            df_mfcc2 = pd.DataFrame(df_mfcc2)\n",
    "            df_bandwith2 = df_bandwith2.iloc[:, :numberOfColumns]\n",
    "            df_loudness2 = df_loudness2.iloc[:, :numberOfColumns]\n",
    "            df_snr2 = df_snr2.iloc[:, :numberOfColumns]\n",
    "            df_zcr2 = df_zcr2.iloc[:, :numberOfColumns]\n",
    "            df_mfcc2 = df_mfcc2.iloc[:, :numberOfColumns]\n",
    "\n",
    "    \n",
    "            df_bandwith2['id']=range(1,len(df_bandwith2)+1)\n",
    "            df_loudness2['id']=range(1,len(df_loudness2)+1)\n",
    "            df_snr2['id']=range(1,len(df_snr2)+1)\n",
    "            df_zcr2['id']=range(1,len(df_zcr2)+1)\n",
    "            df_mfcc2['id']=range(1,len(df_mfcc2)+1)\n",
    "\n",
    "\n",
    "\n",
    "            mergeFirst = pd.merge(df_mfcc2,df_zcr2,on='id')\n",
    "            mergeSecond = pd.merge(mergeFirst, df_loudness2, on='id')\n",
    "            mergeThird = pd.merge(mergeSecond, df_snr2,on='id')\n",
    "            mergeForth = pd.merge(mergeThird, df_bandwith2,on='id')\n",
    "            mergeForth.dropna()\n",
    "            \n",
    "            num_rows = mergeForth.shape[0]\n",
    "  \n",
    "            for i in range(num_rows):\n",
    "                mergeForth.at[i,'id'] = i\n",
    "                i = i+1\n",
    "                mergeForth = mergeForth.rename(columns={'Unnamed: 0': 'ID'})\n",
    "                mergeForth.dropna()\n",
    "                # mergeForth = mergeForth.drop(\"id\", axis=1)\n",
    "                # mergeForth.head()\n",
    "                # mergeForth = mergeForth.sample(frac=1).reset_index(drop=True)\n",
    "                if labelType == \"Frau\" or labelType == \"Mann\":\n",
    "                    mergeForth['label'] = [f'{labelType}']\n",
    "                    mergeForth.to_excel(f\"{destinationPath}{nameOfXLSX}{i}.xlsx\")\n",
    "                    print(mergeForth)\n",
    "\n",
    "            \n",
    "        i+=1\n",
    "\n",
    "\n",
    "                        \n",
    "\n",
    "\n",
    "def mp4_to_wav(mp4_file, wav_file):\n",
    "    audio = AudioSegment.from_file(mp4_file, format=\"mp4\")\n",
    "    audio.export(wav_file, format=\"wav\")\n",
    "\n",
    "\n",
    "\n",
    "def split_wav(wav_file, segment_length=3000):\n",
    "    audio = AudioSegment.from_wav(wav_file)\n",
    "    for i in range(0, len(audio), segment_length):\n",
    "        segment = audio[i:i+segment_length]\n",
    "        segment.export(f\"segment_{i//segment_length}.wav\", format=\"wav\")\n",
    "\n",
    "\n",
    "# get_features_df_excel(\"C:/Users/busse/ManOderFrau/150WS\",\"C:/Users/busse/ManOderFrau/TrainData/\",\"WomenTestDataForMLTraining\",150,\"Frau\")\n",
    "# get_features_df_excel(\"C:/Users/busse/ManOderFrau/MenSequences\",\"C:/Users/busse/ManOderFrau/TrainData/\",\"MenTestDataForMLTraining\",150,\"Mann\")\n",
    "get_features_df_excel(\"C:/Users/busse/ManOderFrau/MenSequencesTest/\",\"menData/\" ,\"X_Men_Test_Data\",2,\"Mann\",10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mp4_to_wav(mp4_file, wav_file):\n",
    "    audio = AudioSegment.from_file(mp4_file, format=\"mp4\")\n",
    "    audio.export(wav_file, format=\"wav\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "get_features_df_excel(\"C:/Users/busse/ManOderFrau/WomenSequences/\",\"C:/Users/busse/ManOderFrau/womenData/\",\"newWomanDataForMLTraining\",150,\"Frau\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2.1 Machine Learning Model trainieren\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "\n",
    "# folder_path = 'C:/Users/busse/ManOderFrau/Woman'  # Pfad zum Ordner mit den CSV-Dateien\n",
    "\n",
    "\n",
    "# for filename in os.listdir(folder_path):\n",
    "#     if filename.endswith('.csv'):\n",
    "#         file_path = os.path.join(folder_path, filename)\n",
    "#         with open(file_path, 'r', newline='', encoding='utf-8') as input_file, open(file_path + '.tmp', 'w', newline='', encoding='utf-8') as output_file:\n",
    "#             reader = csv.reader(input_file)\n",
    "#             writer = csv.writer(output_file, quoting=csv.QUOTE_NONE, escapechar='\\\\')\n",
    "#             for row in reader:\n",
    "#                 writer.writerow(row)\n",
    "#         os.replace(file_path + '.tmp', file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'C:/Users/busse/ManOderFrau/WomenTestXLSX'  # Pfad zum Ordner mit den Excel-Dateien\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.xlsx'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_excel(file_path)\n",
    "        for col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.replace('[', '').str.replace(']', '')\n",
    "        df.to_excel(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path =  'C:/Users/busse/ManOderFrau/WomenTestXLSX'  # Pfad zum Ordner mit den Excel-Dateien\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.xlsx'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_excel(file_path)\n",
    "        df = df[['Unnamed: 0', 'id', 'MFCC', 'Zero Crossing Rate', 'Tonstärke', 'Spektral Kontrast', 'Bandbreite', 'Frau/Mann']]\n",
    "        df.to_excel(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# folder_path = 'C:/Users/busse/ManOderFrau/WomenTestXLSX'  # Pfad zum Ordner mit den CSV-Dateien\n",
    "\n",
    "# for filename in os.listdir(folder_path):\n",
    "#     if filename.endswith('.csv'):\n",
    "#         file_path = os.path.join(folder_path, filename)\n",
    "#         with open(file_path, 'r', newline='', encoding='utf-8') as input_file, open(file_path + '.tmp', 'w', newline='', encoding='utf-8') as output_file:\n",
    "#             reader = csv.reader(input_file)\n",
    "#             writer = csv.writer(output_file, quoting=csv.QUOTE_NONE)\n",
    "#             for row in reader:\n",
    "#                 new_row = [col.replace('\\\\', '') for col in row]\n",
    "#                 writer.writerow(new_row)\n",
    "#         os.replace(file_path + '.tmp', file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordner_path = \"C:/Users/busse/ManOderFrau/Woman/\"\n",
    "\n",
    "# def transformDFforMLModel():\n",
    "#     for file in os.listdir(ordner_path):\n",
    "#         print(file)\n",
    "#         # if file.endswith('.csv'):\n",
    "        #     data = pd.read_csv(ordner_path+file)\n",
    "        #     data['Bandbreite'] = pd.to_numeric(data['Bandbreite'],errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'C:/Users/busse/ManOderFrau/WomenTestXLSX'\n",
    "\n",
    "def merge_excel_data_sets(folder_path,output_file):\n",
    "    folder_path = folder_path\n",
    "    output_file = output_file\n",
    "\n",
    "    xlsx_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.xlsx')]\n",
    "\n",
    "\n",
    "    df = pd.concat((pd.read_excel(f) for f in xlsx_files))\n",
    "\n",
    "\n",
    "    df.to_excel(output_file, index=False)\n",
    "\n",
    "\n",
    "merge_excel_data_sets(\"C:/Users/busse/ManOderFrau/TrainData\",\"TrainData.xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"TrainData.xlsx\",sheet_name=\"Sheet1\")\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "num_rows = df.shape[0]\n",
    "num_rows\n",
    "for i in range(num_rows):\n",
    "    df.at[i,'id'] = i\n",
    "    i = i+1\n",
    "df = df.rename(columns={'Unnamed: 0': 'ID'})\n",
    "df.dropna()\n",
    "df = df.drop(\"id\", axis=1)\n",
    "df.head()\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.to_excel(\"TrainData.xlsx\",index=True)\n",
    "\n",
    "\n",
    "# def shuffle_rows(file_path):\n",
    "\n",
    "#     df = pd.read_excel(file_path)\n",
    "\n",
    "    \n",
    "#     df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#     # Schreiben des DataFrame in die Excel-Datei\n",
    "#     book = load_workbook(file_path)\n",
    "#     writer = pd.ExcelWriter(file_path, engine='openpyxl')\n",
    "#     writer.book = book\n",
    "#     df.to_excel(writer, index=False)\n",
    "#     writer.save()\n",
    "\n",
    "\n",
    "\n",
    "# shuffle_rows(\"C:/Users/busse/ManOderFrau/TrainData.xlsx\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('C:/Users/busse/ManOderFrau/merged2.xlsx')\n",
    "\n",
    "# `id`-Spalte neu nummerieren\n",
    "df = df.reset_index(drop=True)\n",
    "df['id'] = df.index + 1\n",
    "\n",
    "# Ergebnis in eine neue Excel-Datei schreiben\n",
    "df.to_excel('output.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"merged.csv\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('output.xlsx')\n",
    "# # data['Bandbreite'] = pd.to_numeric(data['Bandbreite'],errors='coerce')\n",
    "# # data1 = pd.read_csv(\"C:/Users/busse/ManOderFrau/Woman/Women_Data1.csv\")\n",
    "# # data1\n",
    "# # print(data['Bandbreite'])\n",
    "# X = data.drop(['Frau/Mann','Bandbreite'], axis=1)\n",
    "# X\n",
    "# X = data.drop('Frau/Mann', axis=1)\n",
    "# X.head()\n",
    "# y = data['Frau/Mann']\n",
    "\n",
    "# print(X.dtypes)\n",
    "\n",
    "\n",
    "# print(data['Bandbreite'])\n",
    "# scaler = StandardScaler()\n",
    "# X = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "# # Trainings- und Testdaten aufteilen\n",
    "# X_train, X_test, y_train, y_test = ts(X, y, test_size=0.2)\n",
    "\n",
    "# # Neuronales Netzwerk erstellen\n",
    "# model = Sequential()\n",
    "# model.add(Dense(32, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Neuronales Netzwerk trainieren\n",
    "# model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def preprocess_data(data, model):\n",
    "    X = data.drop([\"label\"], axis=1)\n",
    "    scaler = preprocessing.StandardScaler().fit(X)\n",
    "    X_scaled = scaler.transform(X)\n",
    "    feature_names = model.get_feature_names_out()\n",
    "    X_selected = X_scaled.loc[:, feature_names]\n",
    "    return X_selected\n",
    "\n",
    "def elim_non_equal_columns_excels(excel1, excel2):\n",
    "    data1 = pd.read_excel(excel1)\n",
    "    data2 = pd.read_excel(excel2)\n",
    "\n",
    "    # Find the common columns in both dataframes\n",
    "    common_columns = data1.columns.intersection(data2.columns)\n",
    "\n",
    "    # Select only the common columns from both dataframes\n",
    "    data1_common = data1[common_columns]\n",
    "    data2_common = data2[common_columns]\n",
    "\n",
    "    # Save the resulting dataframes to new Excel files\n",
    "    data1_common.to_excel(excel1, index=False)\n",
    "    data2_common.to_excel(excel2, index=False)\n",
    "\n",
    "\n",
    "\n",
    "# elim_non_equal_columns_excels(\"ReadyData/extractedTestWomanData0.xlsx\",\"C:/Users/busse/ManOderFrau/TrainData.xlsx\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_excel(\"C:/Users/busse/ManOderFrau/TrainData.xlsx\")\n",
    "X = data.drop([\"label\"],axis=1)\n",
    "y = data[\"label\"]\n",
    "dataTest = pd.read_excel('C:/Users/busse/ManOderFrau/menData/X_Men_Test_Data1.xlsx',sheet_name='Sheet1')\n",
    "\n",
    "XdataTest = dataTest.drop([\"label\",\"Unnamed: 0\",\"id\"],axis=1)\n",
    "XdataTest = XdataTest.sample(frac=1)\n",
    "yDataTest = dataTest[\"label\"]\n",
    "scaler1 = preprocessing.StandardScaler().fit(X)\n",
    "scaler2 = preprocessing.StandardScaler().fit(XdataTest)\n",
    "X_scaled = scaler1.transform(X)\n",
    "XdataTest_scaled = scaler2.transform(XdataTest)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2)\n",
    "clf = SVC(kernel='linear')\n",
    "model = clf.fit(X_train, y_train)\n",
    "\n",
    "with open ('ML_Models/modelTrainData.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "\n",
    "\n",
    "# with open('ML_Models/modelTrainData.pkl', 'wb') as f:\n",
    "#         pickle.dump(model, f)\n",
    "\n",
    "\n",
    "y_pred = model.predict(XdataTest)\n",
    "print(classification_report(yDataTest, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "print(y_pred)\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "print('Trainingsgenauigkeit:', train_acc)\n",
    "print('Testgenauigkeit:', test_acc)\n",
    "\n",
    "# print(clf.predict(XdataTest_scaled))\n",
    "\n",
    "\n",
    "# get_features_df_excel(\"C:/Users/busse/ManOderFrau/trance/\",\"extractedTestWomanData\",1,\"Frau\")\n",
    "\n",
    "# y_pred = clf.predict(XdataTest_scaled)\n",
    "# print(classification_report(y_test, y_pred))\n",
    "\n",
    "# with open('model.pkl', 'wb') as f:\n",
    "#         pickle.dump(model, f)\n",
    "\n",
    "\n",
    "# print(y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_features_df_excel(\"WomenSequences/\",\"WomenDataNeuronalesNetz/\",\"newWomanDataForNNTraining\",150,\"0\",10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers  import Dense, LSTM, Dropout\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = pd.read_excel(\"TrainData.xlsx\")\n",
    "data2 = pd.read_excel(\"TrainData/WomenTestDataForMLTraining123.xlsx\")\n",
    "data.dropna()\n",
    "X1= data.drop([\"label\"],axis=1)\n",
    "y1 = data[\"label\"]\n",
    "X2 = data2.drop([\"label\"],axis=1)\n",
    "y2 = data2[\"label\"]\n",
    "\n",
    "\n",
    "# X1 = X1.dropna()\n",
    "# y1 = y1.dropna()\n",
    "# X2 = X2.dropna()\n",
    "# y2 = y2.dropna()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.2)\n",
    "y_train = pd.to_numeric(y_train, errors=\"coerce\")\n",
    "y_test = pd.to_numeric(y_test, errors=\"coerce\")\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(10,), (50,), (100,)],\n",
    "    'learning_rate_init': [0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "model = MLPClassifier(max_iter=1000)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "grid_search.fit(X1, y1)\n",
    "print('Beste Hyperparameter:', grid_search.best_params_)\n",
    "with open(\"ML_Models/MLP.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "for i in range(len(X_test)):\n",
    "    x = X_test.iloc[[i]]\n",
    "    y = model.predict(x)\n",
    "    print(f'x={x}, y={y}')\n",
    "# model = MLPClassifier(hidden_layer_sizes=(50,), activation='relu', solver='adam', learning_rate_init=0.001)\n",
    "\n",
    "# model = model.fit(X1, y1)\n",
    "\n",
    "# y_pred = model.predict(X2)\n",
    "# print(y_pred)\n",
    "# with open (\"ML_Models/MLP.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model, f)\n",
    "# Überprüfen Sie, ob es NaN-Werte in den Trainingsdaten gibt\n",
    "# print(X_train.isna().sum())\n",
    "# print(y_train.isna().sum())\n",
    "\n",
    "# Überprüfen Sie, ob es NaN-Werte in den Testdaten gibt\n",
    "# print(X_test.isna().sum())\n",
    "# print(y_test.isna().sum())\n",
    "\n",
    "\n",
    "    \n",
    "# # model_pred_X1 =model.predict(X1)\n",
    "\n",
    "#     model_pred_X2 = model.predict(X2)\n",
    "#     # print(model_pred_X1)\n",
    "#     # print(model_pred_X2)\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    print(y_test_pred,y_train_pred)\n",
    "    y_train_pred\n",
    "    train_acc = accuracy_score(y_train,y_train_pred)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)  \n",
    "    print('Trainingsgenauigkeit:', train_acc)\n",
    "    print('Testgenauigkeit:', test_acc)\n",
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# # Die Größe der Trainings- und Testsets ausgeben\n",
    "# print('Trainingsset:', X_train.shape[0])\n",
    "# print('Testset:', X_test.shape[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3.1 k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Laden Sie Ihre Daten hier\n",
    "# X = ...\n",
    "# y = ...\n",
    "\n",
    "# Teilen Sie die Daten in Trainings- und Testsets auf\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Erstellen Sie das k-NN-Modell\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Trainieren Sie das Modell mit den Trainingsdaten\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen für das Testset treffen\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Berechnen Sie die Genauigkeit des Modells\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Genauigkeit:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers  import Dense, LSTM, Dropout\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# with open ('ML_Models/modelTrainData.pkl', 'rb') as f:\n",
    "#         model = pickle.load(f)\n",
    "\n",
    "data = pd.read_excel(\"merged2.xlsx\")\n",
    "data2 = pd.read_excel(\"TrainData/WomenTestDataForMLTraining123.xlsx\")\n",
    "# Lade das vorbereitete Dataset\n",
    "X1= data.drop([\"label\"],axis=1)\n",
    "y1 = data[\"label\"]\n",
    "X2 = data2.drop([\"label\"],axis=1)\n",
    "y2 = data2[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.2)\n",
    "y_train = pd.to_numeric(y_train, errors=\"coerce\")\n",
    "y_test = pd.to_numeric(y_test, errors=\"coerce\")\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_shape=(X1.shape[1],), activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "optimizer = Adam(learning_rate = 0.001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Erstellen Sie ein EarlyStopping-Callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# Trainieren Sie das Modell mit dem EarlyStopping-Callback\n",
    "history = model.fit(X_train, y_train, epochs=1000, batch_size=32,\n",
    "                validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "y_pred = model.predict(X1)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "\n",
    "\n",
    "with open('ML_Models/NeuronalesNetz.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "# print(y_pred[:10])\n",
    "\n",
    "# Zugriff auf die Genauigkeitswerte\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "# Die Genauigkeitswerte ausgeben\n",
    "print('Trainingsgenauigkeit:', acc)\n",
    "print('Validierungsgenauigkeit:', val_acc)\n",
    "\n",
    "\n",
    "# print(y_pred[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"output.xlsx\")\n",
    "data\n",
    "\n",
    "X = data.drop(['Frau/Mann','Bandbreite'], axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Lese die Trainingsdaten aus einer Excel-Datei\n",
    "df = pd.read_excel('C:/Users/busse/ManOderFrau/MegedDataWomen.xlsx')\n",
    "X = df.iloc[:, :-1].astype(str).apply(lambda x: x.str.split(',').explode()).values.astype(float)\n",
    "# Wandle die Daten in NumPy-Arrays um\n",
    "X = df.iloc[:, :-1].apply(lambda x: x.str.split(',').explode()).values.astype(float)\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "# Erstelle das Random Forest-Modell\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "# Trainiere das Random Forest-Modell\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Teste das Random Forest-Modell\n",
    "print(clf.predict([[0, 0, 0], [1, 1, 1]]))\n",
    "\n",
    "# Lese die Trainingsdaten aus einer Excel-Datei\n",
    "# df = pd.read_excel('cleaned_data.xlsx')\n",
    "# X = df.iloc[:, :-1].astype(str).apply(lambda x: x.str.split(',').explode()).values.astype(float)\n",
    "# Wandle die Daten in NumPy-Arrays um\n",
    "# X = df.iloc[:, :-1].apply(lambda x: x.str.split(',').explode()).values.astype(float)\n",
    "# y = df.iloc[:, -1].values\n",
    "\n",
    "# # Erstelle das neuronale Netz\n",
    "# clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "#                     hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "# # Trainiere das neuronale Netz\n",
    "# clf.fit(X, y)\n",
    "\n",
    "# # Teste das neuronale Netz\n",
    "# print(clf.predict([[0, 0, 0], [1, 1, 1]]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Lese die Trainingsdaten aus einer Excel-Datei\n",
    "# df = pd.read_excel('output.xlsx')\n",
    "# df['Bandbreite'] = pd.to_numeric(data['Bandbreite'],errors='coerce')\n",
    "# df['MFCC'] = pd.to_numeric(data['MFCC'],errors='coerce')\n",
    "# df['Zero Crossing Rate'] = pd.to_numeric(data['Zero Crossing Rate'],errors='coerce')\n",
    "# df['Tonstärke'] = pd.to_numeric(data['Tonstärke'],errors='coerce')\n",
    "# df['Spektral Kontrast'] = pd.to_numeric(data['Spektral Kontrast'],errors='coerce')\n",
    "\n",
    "# df\n",
    "# # Wandle die Daten in NumPy-Arrays um\n",
    "# X = df[['MFCC', 'Zero Crossing Rate','Tonstärke','Spektral Kontrast','Bandbreite']].values\n",
    "# X = df.drop(['Zero Crossing Rate','Tonstärke','Spektral Kontrast','Bandbreite'], axis=1)\n",
    "# y = df['Frau/Mann'].values\n",
    "\n",
    "# # Wandle die Daten in NumPy-Arrays um\n",
    "# X = df.iloc[:, :-1].values\n",
    "# y = df.iloc[:, -1].values\n",
    "\n",
    "# # Erstelle das neuronale Netz\n",
    "# clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "#                     hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "# # Trainiere das neuronale Netz\n",
    "# clf.fit(X, y)\n",
    "\n",
    "# # Teste das neuronale Netz\n",
    "# print(clf.predict([[0, 0, 0], [1, 1, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_cells(file_name):\n",
    "    # Lese die Daten aus der Excel-Datei\n",
    "    df = pd.read_excel(file_name)\n",
    "\n",
    "    # Erstelle einen neuen DataFrame, um die bereinigten Daten zu speichern\n",
    "    new_df = pd.DataFrame()\n",
    "\n",
    "    # Gehe durch jede Spalte im DataFrame\n",
    "    for col in df.columns:\n",
    "        # Wandle die Zellen in der Spalte in Listen um\n",
    "        data = df[col].str.split(',')\n",
    "\n",
    "        # Finde die maximale Anzahl von Elementen in den Listen\n",
    "        max_len = data.str.len().max()\n",
    "\n",
    "        # Erstelle neue Spalten für jedes Element in den Listen\n",
    "        for i in range(max_len):\n",
    "            new_col_name = f'{col}_{i}'\n",
    "            new_df[new_col_name] = data.str[i]\n",
    "\n",
    "    # Speichere die bereinigten Daten in einer neuen Excel-Datei\n",
    "    new_df.to_excel('split_data.xlsx', index=False)\n",
    "\n",
    "# Verwende die Funktion, um die Daten aus den Zellen einer Excel-Datei zu lesen und in neue Spalten zu schreiben\n",
    "split_cells('output.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def remove_duplicate_labels(file_name):\n",
    "    # Lese die Daten aus der Excel-Datei\n",
    "    df = pd.read_excel(file_name)\n",
    "\n",
    "    # Finde doppelte Labels\n",
    "    duplicates = df.columns[df.columns.duplicated()]\n",
    "\n",
    "    # Entferne doppelte Labels\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "    # Speichere die bereinigten Daten in einer neuen Excel-Datei\n",
    "    df.to_excel('cleaned_data.xlsx', index=False)\n",
    "\n",
    "    # Gebe die Anzahl der entfernten doppelten Labels zurück\n",
    "    return len(duplicates)\n",
    "\n",
    "# Verwende die Funktion, um doppelte Labels aus einer Excel-Datei zu entfernen\n",
    "num_duplicates = remove_duplicate_labels('output.xlsx')\n",
    "print(f'Es wurden {num_duplicates} doppelte Labels entfernt.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not ready "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# def extract_feature(file_name):\n",
    "#     x, sample_rate = sf.read(file_name)\n",
    "#     stft = np.abs(librosa.stft(x))\n",
    "#     mfccs = np.mean(librosa.feature.mfcc(y=x, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "#     chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
    "#     mel = np.mean(librosa.feature.melspectrogram(x, sr=sample_rate).T, axis=0)\n",
    "#     contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T, axis=0)\n",
    "#     tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(x), sr=sample_rate).T, axis=0)\n",
    "    \n",
    "#     features = np.hstack([mfccs, chroma, mel, contrast, tonnetz])\n",
    "#     df = pd.DataFrame(features).T\n",
    "    \n",
    "#     if not os.path.isfile(csv_file):\n",
    "#         df.to_csv(csv_file, index=False)\n",
    "#     else:\n",
    "#         df.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# directory = 'rock'\n",
    "# csv_file = 'woman.csv'\n",
    "\n",
    "# for filename in os.listdir(directory):\n",
    "#     if filename.endswith('.wav'):\n",
    "#         print(filename)\n",
    "#         extract_feature(filename, filename)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAN_FILE = wave.open(\"Microphone.wav\", \"r\")\n",
    "# NO_MAN = wave.open(\"FemaleTalkGibber.wav\", \"r\")\n",
    "# MAN = wave.open(\"Microphone.wav\", \"r\")\n",
    "# MAN_FILE_samplerate = MAN_FILE.getframerate()\n",
    "# NO_MAN_samplerate = NO_MAN.getframerate()\n",
    "# print(\"Framerate MAN_FILE\" \":\" + str(MAN_FILE_samplerate))\n",
    "# print(\"Framerate NO_MAN\" \":\" + str(NO_MAN_samplerate))\n",
    "# df = pd.read_csv(\"mergeddfs.csv\")\n",
    "# df[(df != 0).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal = MAN_FILE.readframes(-1)\n",
    "# signal = np.frombuffer(signal, \"int16\")\n",
    "# signal2 = NO_MAN.readframes(-1)\n",
    "# signal2 = np.frombuffer(signal2, \"int16\")\n",
    "# if MAN_FILE.getnchannels() == 2:\n",
    "#     print(\"Just mono files\")\n",
    "#     sys.exit(0)\n",
    "\n",
    "# plt.figure(1)\n",
    "# plt.title(\"Man_WAVE\")\n",
    "# plt.plot(signal)\n",
    "# plt.plot(signal2)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# num_segments = len(signal) // MAN_FILE_samplerate\n",
    "\n",
    "# amplitudes = []\n",
    "# seconds = []\n",
    "\n",
    "# for i in range(num_segments):\n",
    "#     start = i * MAN_FILE_samplerate\n",
    "#     end = start + MAN_FILE_samplerate\n",
    "#     segment = signal[start:end]\n",
    "#     amplitude = max(segment)\n",
    "#     amplitudes.append(amplitude)\n",
    "#     seconds.append(i)\n",
    "# df = pd.DataFrame({'Amplitude': amplitudes})\n",
    "# print(df)\n",
    "# df.to_csv(\"Amplitude_Seconds_MAN_FILE.csv\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "num_segments = len(signal) // MAN_FILE_samplerate\n",
    "\n",
    "\n",
    "num_segments = (num_segments // 5) * 5\n",
    "\n",
    "\n",
    "amplitudes = [[] for _ in range(5)]\n",
    "seconds = []\n",
    "\n",
    "for i in range(num_segments):\n",
    "    start = i * MAN_FILE_samplerate\n",
    "    end = start + MAN_FILE_samplerate\n",
    "    segment = signal[start:end]\n",
    "    amplitude = max(segment)\n",
    "    amplitudes[i % 5].append(amplitude)\n",
    "    if i % 5 == 0:\n",
    "        seconds.append(i // 5)\n",
    "\n",
    "df_amplitude = pd.DataFrame({'Sekunde': seconds,\n",
    "                   'Amplitude1': amplitudes[0],\n",
    "                   'Amplitude2': amplitudes[1],\n",
    "                   'Amplitude3': amplitudes[2],\n",
    "                   'Amplitude4': amplitudes[3],\n",
    "                   'Amplitude5': amplitudes[4]})\n",
    "\n",
    "df_amplitude.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_feauture_extraction = pd.read_csv('Amplitude_Seconds_MAN_FILE.csv')\n",
    "# df_feauture_extraction.index = pd.MultiIndex.from_arrays([df.index.get_level_values(i).astype('object') for i in range(df.index.nlevels)])\n",
    "# print(df_feauture_extraction.index)\n",
    "# df_feauture_extraction.head()\n",
    "# features = extract_features(df_feauture_extraction, column_id=\"Unnamed: 0\", column_sort=\"Unnamed: 0\")\n",
    "# # features = extract_relevant_features(timeseries, y, column_id=\"id\", column_sort=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samplerate, data = wavfile.read('Microphone.wav')\n",
    "# plt.specgram(data, Fs=samplerate)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_man, sr_man = librosa.load(\"Microphone.wav\")\n",
    "# mfccs_man = librosa.feature.mfcc(y=y_man, sr = sr_man)\n",
    "# labels_man = ['Mann' for mfcc in mfccs_man] \n",
    "\n",
    "# folder = 'rock'\n",
    "# y_combined = np.array([])\n",
    "# sr_combined = None\n",
    "\n",
    "# for file in os.listdir(folder):\n",
    "#     if file.endswith('.wav'):\n",
    "#         y, sr = librosa.load(os.path.join(folder, file), sr=None)\n",
    "#         y_combined = np.concatenate((y_combined, y))\n",
    "#         sr_combined = sr\n",
    "\n",
    "\n",
    "\n",
    "# # Überprüfen der Größe des kombinierten Signals\n",
    "# print(f'Size of combined signal: {len(y_combined)}')\n",
    "\n",
    "# output_file = 'Woman4.wav'\n",
    "# sf.write(output_file, y_combined, sr_combined)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_woman, sr_woman = librosa.load(\"Women1.wav\")\n",
    "# mfccs_woman = librosa.feature.mfcc(y=y_woman, sr = sr_woman)\n",
    "# labels_woman = ['Frau' for mfcc in mfccs_woman] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mfccs_df_man = pd.DataFrame(mfccs_man)\n",
    "# mfccs_df_man['labels'] = labels_man\n",
    "# mfccs_df_man.head(n=100)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# mfccs_df_woman = pd.DataFrame(mfccs_woman)\n",
    "# mfccs_df_woman['labels'] = labels_woman\n",
    "# mfccs_df_woman.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_mfccs_df = pd.concat([mfccs_df_man, mfccs_df_woman], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_mfccs_nan_df = merged_mfccs_df.fillna(value = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mfccs_male = []\n",
    "# male_files = ['MAX_liest_deutsches_Alphabet.wav', 'Jakob_liest_deutschesAlphabet.wav','paul_liest_deutschesAlphabet.wav']\n",
    "# for file in male_files:\n",
    "#     y, sr = librosa.load(file, sr=None)\n",
    "#     mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "#     mfccs_male.append(mfcc)\n",
    "\n",
    "\n",
    "\n",
    "# # mfccs_female = []\n",
    "# # for file in female_files:\n",
    "# #     y, sr = librosa.load(file, sr=None)\n",
    "# #     mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "# #     mfccs_female.append(mfcc)\n",
    "\n",
    "\n",
    "# print(mfccs_male)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mfccs_female = []\n",
    "# folder = 'rock'\n",
    "# for i in os.listdir(folder):\n",
    "#     y, sr = librosa.load(i, sr=None)\n",
    "#     mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "#     mfccs_female.append(mfcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = merged_mfccs_nan_df.drop('labels', axis=1)\n",
    "# y = merged_mfccs_nan_df['labels']\n",
    "\n",
    "# X_train, X_test, y_train, y_test = ts(X, y, test_size=0.2)\n",
    "# assert len(X) == len(y)\n",
    "# model = svc()\n",
    "# model.fit(X_train, y_train)\n",
    "# accuracy = model.score(X_test, y_test)\n",
    "# print(f'Accuracy: {accuracy}') \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
